{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qHtO18WB9y7P"
   },
   "source": [
    "\n",
    "<a href=\"https://colab.research.google.com/github/aviadr1/learn-advanced-python/blob/master/content/08_test_driven_development/08-pytest.ipynb\" target=\"_blank\">\n",
    "<img src=\"https://colab.research.google.com/assets/colab-badge.svg\" \n",
    "     title=\"Open this file in Google Colab\" alt=\"Colab\"/>\n",
    "</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "awpj1eiRUuKM"
   },
   "source": [
    "# useful resources:\n",
    "1. https://stackabuse.com/test-driven-development-with-pytest/\n",
    "2. https://docs.pytest.org/en/latest/goodpractices.html#conventions-for-python-test-discovery\n",
    "3. https://blog.ionelmc.ro/2014/05/25/python-packaging/#the-structure\n",
    "4. https://github.com/vanzaj/tdd-pytest/blob/master/docs/tdd-pytest/content/tdd-basics.md\n",
    "5. https://opensource.com/article/18/6/pytest-plugins\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GPIt7vgv97Vu"
   },
   "source": [
    "# setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pd8QNG1J9y7U"
   },
   "source": [
    "1. install `pytest`\n",
    "2. install `pytest-sugar` which will give us nicer output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-OAK0map9y7X"
   },
   "outputs": [],
   "source": [
    "pip -q install pytest pytest-sugar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "PvUWrFRfV33t",
    "outputId": "d5ef9515-19e9-4306-ebd6-ecb5a1665d18"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/content/tdd/tdd'"
      ]
     },
     "execution_count": 40,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# move to tdd directory\n",
    "from pathlib import Path\n",
    "if Path.cwd().name != 'tdd':\n",
    "    %mkdir tdd\n",
    "    %cd tdd\n",
    "\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WrRF4Qh2W_Xk"
   },
   "outputs": [],
   "source": [
    "# cleanup all files\n",
    "%rm *.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WB1QY4sbPs4-"
   },
   "source": [
    "# How pytest discovers tests\n",
    "\n",
    "pytests uses the following [conventions](https://docs.pytest.org/en/latest/goodpractices.html#conventions-for-python-test-discovery) to automatically discovering tests:\n",
    "  1. files with tests should be called `test_*.py` or `*_test.py `\n",
    "  2. test function name should start with `test_`\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uJjWla1qxd3i"
   },
   "source": [
    "# our first test\n",
    "to see if our code works, we can use the `assert` python keyword. pytest adds hooks to assertions to make them more useful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "9acpZigRVANF",
    "outputId": "fb8add6a-cc2c-4ed5-887d-97d51c8f094d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing test_math.py\n"
     ]
    }
   ],
   "source": [
    "%%file test_math.py\n",
    "\n",
    "import math\n",
    "def test_add():\n",
    "    assert 1+1 == 2\n",
    "\n",
    "def test_mul():\n",
    "    assert 6*7 == 42\n",
    "\n",
    "def test_sin():\n",
    "    assert math.sin(0) == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XTBnZ3-vVe0p"
   },
   "source": [
    "now lets run pytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 155
    },
    "colab_type": "code",
    "id": "DXBQkMc8VeD8",
    "outputId": "b57ff406-b11d-4635-d2b1-a06827bea498"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mTest session starts (platform: linux, Python 3.6.9, pytest 5.3.5, pytest-sugar 0.9.2)\u001b[0m\n",
      "rootdir: /content\n",
      "plugins: sugar-0.9.2, xdist-1.31.0, forked-1.1.3, timeout-1.3.4\n",
      "\u001b[1mcollecting ... \u001b[0m\n",
      " \u001b[36m\u001b[0mtest_math.py\u001b[0m \u001b[32m✓\u001b[0m\u001b[32m✓\u001b[0m\u001b[32m✓\u001b[0m                                                \u001b[32m100% \u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m██\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m██\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m███\u001b[0m\n",
      "\n",
      "Results (0.02s):\n",
      "\u001b[32m       3 passed\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python -m pytest test_math.py "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D2x5SB05XNAF"
   },
   "source": [
    "Great! we just wrote 3 tests that shows that basic math still works\n",
    "\n",
    "Hurray!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CKfU1dK3YMLQ"
   },
   "source": [
    "## your turn\n",
    "\n",
    "write a test for the following function. \n",
    "\n",
    "if there is a bug in the function, fix it\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "nebAftmgbo-X",
    "outputId": "9e774bc9-b990-406c-a525-7bfb3259ca0b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing make_triangle.py\n"
     ]
    }
   ],
   "source": [
    "%%file make_triangle.py\n",
    "\n",
    "# version 1\n",
    "\n",
    "def make_triangle(n):\n",
    "    \"\"\"\n",
    "    draws a triangle using '@' letters\n",
    "    for instance:\n",
    "        >>> print('\\n'.join(make_triangle(3))\n",
    "        @\n",
    "        @@\n",
    "        @@@\n",
    "    \"\"\"\n",
    "\n",
    "    for i in range(n):\n",
    "        yield '@' * i\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WWIGFnRQcNUc"
   },
   "source": [
    "## solution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "rWHxUDs3cYSr",
    "outputId": "cdc052ae-32ee-4275-8333-dab8d7512d7e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting test_make_triangle.py\n"
     ]
    }
   ],
   "source": [
    "%%file test_make_triangle.py\n",
    "\n",
    "from make_triangle import make_triangle\n",
    "\n",
    "def test_make_triangle():\n",
    "    expected = \"@\"\n",
    "    actual = '\\n'.join(make_triangle(1))\n",
    "    assert actual == expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 380
    },
    "colab_type": "code",
    "id": "IInhHAiIc0ft",
    "outputId": "610e4a4a-99c3-4038-a7fb-a34d5595d24e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mTest session starts (platform: linux, Python 3.6.9, pytest 5.3.5, pytest-sugar 0.9.2)\u001b[0m\n",
      "rootdir: /content\n",
      "plugins: sugar-0.9.2, xdist-1.31.0, forked-1.1.3, timeout-1.3.4\n",
      "\u001b[1mcollecting ... \u001b[0m\n",
      "\n",
      "―――――――――――――――――――――――――――――― test_make_triangle ――――――――――――――――――――――――――――――\n",
      "\n",
      "\u001b[1m    def test_make_triangle():\u001b[0m\n",
      "\u001b[1m        expected = \"@\"\u001b[0m\n",
      "\u001b[1m        actual = '\\n'.join(make_triangle(1))\u001b[0m\n",
      "\u001b[1m>       assert actual == expected\u001b[0m\n",
      "\u001b[1m\u001b[31mE       AssertionError: assert '' == '@'\u001b[0m\n",
      "\u001b[1m\u001b[31mE         + @\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mtest_make_triangle.py\u001b[0m:7: AssertionError\n",
      "\n",
      " \u001b[36m\u001b[0mtest_make_triangle.py\u001b[0m \u001b[31m⨯\u001b[0m                                         \u001b[31m100% \u001b[0m\u001b[40m\u001b[31m█\u001b[0m\u001b[40m\u001b[31m█████████\u001b[0m\n",
      "\n",
      "Results (0.04s):\n",
      "\u001b[31m       1 failed\u001b[0m\n",
      "         - \u001b[36m\u001b[0mtest_make_triangle.py\u001b[0m:4 \u001b[31mtest_make_triangle\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python -m pytest test_make_triangle.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CmU6gxq2hc77"
   },
   "source": [
    "so the expected starts with `'@'` and the actual starts with `''` ...\n",
    "\n",
    "this is a bug! lets fix the code and re-run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "opFtkK3WhtPZ",
    "outputId": "26a8bc29-99c1-44d6-8c3e-27ac78ba81ef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting make_triangle.py\n"
     ]
    }
   ],
   "source": [
    "%%file make_triangle.py\n",
    "\n",
    "# version 2 \n",
    "def make_triangle(n):\n",
    "    \"\"\"\n",
    "    draws a triangle using '@' letters\n",
    "    for instance:\n",
    "        >>> print('\\n'.join(make_triangle(3))\n",
    "        @\n",
    "        @@\n",
    "        @@@\n",
    "    \"\"\"\n",
    "\n",
    "    for i in range(1, n+1):\n",
    "        yield '@' * i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 155
    },
    "colab_type": "code",
    "id": "6nw_HYIjh2qh",
    "outputId": "8096fd15-0a38-4584-c8c3-41b74769db94"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.6.9, pytest-3.6.4, py-1.8.1, pluggy-0.7.1\n",
      "rootdir: /content, inifile:\n",
      "collected 1 item                                                               \u001b[0m\u001b[1m\n",
      "\n",
      "test_make_triangle.py .\u001b[36m                                                  [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m\u001b[1m=========================== 1 passed in 0.01 seconds ===========================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python -m pytest test_make_triangle.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "el2LYRdW0qZ6"
   },
   "source": [
    "# Pytest context-sensitive comparisons\n",
    "[Reference](https://docs.pytest.org/en/3.0.1/assert.html#making-use-of-context-sensitive-comparisons)\n",
    "\n",
    "pytest has rich support for providing context-sensitive information when it encounters comparisons. \n",
    "\n",
    "Special comparisons are done for a number of cases:\n",
    "- comparing long strings: a context diff is shown\n",
    "- comparing long sequences: first failing indices\n",
    "- comparing dicts: different entries\n",
    "\n",
    "Here's how this looks like for set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "6zC5qxb91HGL",
    "outputId": "4f96392c-5834-40b4-d3fd-764166044820"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing test_compare_fruits.py\n"
     ]
    }
   ],
   "source": [
    "%%file test_compare_fruits.py\n",
    "def test_set_comparison():\n",
    "    set1 = set(['Apples', 'Bananas', 'Watermelon', 'Pear',  'Guave', 'Carambola', 'Plum'])\n",
    "    set2 = set(['Plum', 'Apples', 'Grapes', 'Watermelon','Pear', 'Guave', 'Carambola',  'Melon' ])\n",
    "    assert set1 == set2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 466
    },
    "colab_type": "code",
    "id": "rcymuKrV11Pz",
    "outputId": "70300763-6636-4b3b-b36f-44b5f900876b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mTest session starts (platform: linux, Python 3.6.9, pytest 5.3.5, pytest-sugar 0.9.2)\u001b[0m\n",
      "rootdir: /content\n",
      "plugins: sugar-0.9.2, xdist-1.31.0, forked-1.1.3, timeout-1.3.4\n",
      "\u001b[1mcollecting ... \u001b[0m\n",
      "\n",
      "――――――――――――――――――――――――――――― test_set_comparison ――――――――――――――――――――――――――――――\n",
      "\n",
      "\u001b[1m    def test_set_comparison():\u001b[0m\n",
      "\u001b[1m        set1 = set(['Apples', 'Bananas', 'Watermelon', 'Pear',  'Guave', 'Carambola', 'Plum'])\u001b[0m\n",
      "\u001b[1m        set2 = set(['Plum', 'Apples', 'Grapes', 'Watermelon','Pear', 'Guave', 'Carambola',  'Melon' ])\u001b[0m\n",
      "\u001b[1m>       assert set1 == set2\u001b[0m\n",
      "\u001b[1m\u001b[31mE       AssertionError: assert {'Apples', 'B..., 'Plum', ...} == {'Apples', 'C..., 'Pear', ...}\u001b[0m\n",
      "\u001b[1m\u001b[31mE         Extra items in the left set:\u001b[0m\n",
      "\u001b[1m\u001b[31mE         'Bananas'\u001b[0m\n",
      "\u001b[1m\u001b[31mE         Extra items in the right set:\u001b[0m\n",
      "\u001b[1m\u001b[31mE         'Melon'\u001b[0m\n",
      "\u001b[1m\u001b[31mE         'Grapes'\u001b[0m\n",
      "\u001b[1m\u001b[31mE         Use -v to get the full diff\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mtest_compare_fruits.py\u001b[0m:4: AssertionError\n",
      "\n",
      " \u001b[36m\u001b[0mtest_compare_fruits.py\u001b[0m \u001b[31m⨯\u001b[0m                                        \u001b[31m100% \u001b[0m\u001b[40m\u001b[31m█\u001b[0m\u001b[40m\u001b[31m█████████\u001b[0m\n",
      "\n",
      "Results (0.03s):\n",
      "\u001b[31m       1 failed\u001b[0m\n",
      "         - \u001b[36m\u001b[0mtest_compare_fruits.py\u001b[0m:1 \u001b[31mtest_set_comparison\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python -m pytest test_compare_fruits.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F8M63k6b6F12"
   },
   "source": [
    "## your turn\n",
    "\n",
    "test the following function `count_words()` and fix any bugs.\n",
    "\n",
    "the expected output from the function is given in `expected_output`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mJRTUb1J9-t8"
   },
   "outputs": [],
   "source": [
    "expected_output = {\n",
    " 'and': 2,\n",
    " 'chief': 2,\n",
    " 'didnt': 1,\n",
    " 'efficiency': 1,\n",
    " 'expected': 1,\n",
    " 'expects': 1,\n",
    " 'fear': 2,\n",
    " 'i': 1,\n",
    " 'inquisition': 2,\n",
    " 'is': 1,\n",
    " 'no': 1,\n",
    " 'one': 1,\n",
    " 'our': 1,\n",
    " 'ruthless': 1,\n",
    " 'spanish': 2,\n",
    " 'surprise': 3,\n",
    " 'the': 2,\n",
    " 'two': 1,\n",
    " 'weapon': 1,\n",
    " 'weapons': 1,\n",
    " 'well': 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "lkSdtFl96HN-",
    "outputId": "b5e7d548-4f16-4cce-b4e0-febffdbff903"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting spanish_inquisition.py\n"
     ]
    }
   ],
   "source": [
    "%%file spanish_inquisition.py\n",
    "# version 1: buggy\n",
    "import collections\n",
    "\n",
    "quote = \"\"\"\n",
    "Well, I didn't expected the Spanish Inquisition ...\n",
    "No one expects the Spanish Inquisition!\n",
    "Our chief weapon is surprise, fear and surprise;\n",
    "two chief weapons, fear, surprise, and ruthless efficiency! \n",
    "\"\"\"\n",
    "\n",
    "def remove_punctuation(quote):\n",
    "    quote.translate(str.maketrans('', '', \"',.!?;\")).lower()\n",
    "    return quote\n",
    "\n",
    "def count_words(quote):\n",
    "    quote = remove_punctuation(quote)\n",
    "    return dict(collections.Counter(quote.split(' ')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gwucG2PJ91xT"
   },
   "source": [
    "## solution\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "EOVhaPZu_J8f",
    "outputId": "a9f4496e-1ac6-473e-b4c1-355f70faf655"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting test_spanish_inquisition.py\n"
     ]
    }
   ],
   "source": [
    "%%file test_spanish_inquisition.py\n",
    "\n",
    "from spanish_inquisition import *\n",
    "\n",
    "expected_output = {\n",
    " 'and': 2,\n",
    " 'chief': 2,\n",
    " 'didnt': 1,\n",
    " 'efficiency': 1,\n",
    " 'expected': 1,\n",
    " 'expects': 1,\n",
    " 'fear': 2,\n",
    " 'i': 1,\n",
    " 'inquisition': 2,\n",
    " 'is': 1,\n",
    " 'no': 1,\n",
    " 'one': 1,\n",
    " 'our': 1,\n",
    " 'ruthless': 1,\n",
    " 'spanish': 2,\n",
    " 'surprise': 3,\n",
    " 'the': 2,\n",
    " 'two': 1,\n",
    " 'weapon': 1,\n",
    " 'weapons': 1,\n",
    " 'well': 1}\n",
    "\n",
    "def test_spanish_inquisition():\n",
    "    actual = count_words(quote)\n",
    "    assert actual == expected_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "-ZE9GzGA_hjW",
    "outputId": "e8fb06c1-484c-4043-9fc2-2c4236fbf8ae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mTest session starts (platform: linux, Python 3.6.9, pytest 5.3.5, pytest-sugar 0.9.2)\u001b[0m\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /content\n",
      "plugins: sugar-0.9.2, xdist-1.31.0, forked-1.1.3, timeout-1.3.4\n",
      "\u001b[1mcollecting ... \u001b[0m\n",
      "\n",
      "――――――――――――――――――――――――――― test_spanish_inquisition ―――――――――――――――――――――――――――\n",
      "\n",
      "\u001b[1m    def test_spanish_inquisition():\u001b[0m\n",
      "\u001b[1m        actual = count_words(quote)\u001b[0m\n",
      "\u001b[1m>       assert actual == expected_output\u001b[0m\n",
      "\u001b[1m\u001b[31mE       assert {'\\n': 1,\\n '\\nWell,': 1,\\n '...\\nNo': 1,\\n 'I': 1,\\n 'Inquisition': 1,\\n 'Inquisition!\\nOur': 1,\\n 'Spanish': 2,\\n 'and': 2,\\n 'chief': 2,\\n \"didn't\": 1,\\n 'efficiency!': 1,\\n 'expected': 1,\\n 'expects': 1,\\n 'fear': 1,\\n 'fear,': 1,\\n 'is': 1,\\n 'one': 1,\\n 'ruthless': 1,\\n 'surprise,': 2,\\n 'surprise;\\ntwo': 1,\\n 'the': 2,\\n 'weapon': 1,\\n 'weapons,': 1} == {'and': 2,\\n 'chief': 2,\\n 'didnt': 1,\\n 'efficiency': 1,\\n 'expected': 1,\\n 'expects': 1,\\n 'fear': 2,\\n 'i': 1,\\n 'inquisition': 2,\\n 'is': 1,\\n 'no': 1,\\n 'one': 1,\\n 'our': 1,\\n 'ruthless': 1,\\n 'spanish': 2,\\n 'surprise': 3,\\n 'the': 2,\\n 'two': 1,\\n 'weapon': 1,\\n 'weapons': 1,\\n 'well': 1}\u001b[0m\n",
      "\u001b[1m\u001b[31mE         Common items:\u001b[0m\n",
      "\u001b[1m\u001b[31mE         {'and': 2,\u001b[0m\n",
      "\u001b[1m\u001b[31mE          'chief': 2,\u001b[0m\n",
      "\u001b[1m\u001b[31mE          'expected': 1,\u001b[0m\n",
      "\u001b[1m\u001b[31mE          'expects': 1,\u001b[0m\n",
      "\u001b[1m\u001b[31mE          'is': 1,\u001b[0m\n",
      "\u001b[1m\u001b[31mE          'one': 1,\u001b[0m\n",
      "\u001b[1m\u001b[31mE          'ruthless': 1,\u001b[0m\n",
      "\u001b[1m\u001b[31mE          'the': 2,\u001b[0m\n",
      "\u001b[1m\u001b[31mE          'weapon': 1}\u001b[0m\n",
      "\u001b[1m\u001b[31mE         Differing items:\u001b[0m\n",
      "\u001b[1m\u001b[31mE         {'fear': 1} != {'fear': 2}\u001b[0m\n",
      "\u001b[1m\u001b[31mE         Left contains 13 more items:\u001b[0m\n",
      "\u001b[1m\u001b[31mE         {'\\n': 1,\u001b[0m\n",
      "\u001b[1m\u001b[31mE          '\\nWell,': 1,\u001b[0m\n",
      "\u001b[1m\u001b[31mE          '...\\nNo': 1,\u001b[0m\n",
      "\u001b[1m\u001b[31mE          'I': 1,\u001b[0m\n",
      "\u001b[1m\u001b[31mE          'Inquisition': 1,\u001b[0m\n",
      "\u001b[1m\u001b[31mE          'Inquisition!\\nOur': 1,\u001b[0m\n",
      "\u001b[1m\u001b[31mE          'Spanish': 2,\u001b[0m\n",
      "\u001b[1m\u001b[31mE          \"didn't\": 1,\u001b[0m\n",
      "\u001b[1m\u001b[31mE          'efficiency!': 1,\u001b[0m\n",
      "\u001b[1m\u001b[31mE          'fear,': 1,\u001b[0m\n",
      "\u001b[1m\u001b[31mE          'surprise,': 2,\u001b[0m\n",
      "\u001b[1m\u001b[31mE          'surprise;\\ntwo': 1,\u001b[0m\n",
      "\u001b[1m\u001b[31mE          'weapons,': 1}\u001b[0m\n",
      "\u001b[1m\u001b[31mE         Right contains 11 more items:\u001b[0m\n",
      "\u001b[1m\u001b[31mE         {'didnt': 1,\u001b[0m\n",
      "\u001b[1m\u001b[31mE          'efficiency': 1,\u001b[0m\n",
      "\u001b[1m\u001b[31mE          'i': 1,\u001b[0m\n",
      "\u001b[1m\u001b[31mE          'inquisition': 2,\u001b[0m\n",
      "\u001b[1m\u001b[31mE          'no': 1,\u001b[0m\n",
      "\u001b[1m\u001b[31mE          'our': 1,\u001b[0m\n",
      "\u001b[1m\u001b[31mE          'spanish': 2,\u001b[0m\n",
      "\u001b[1m\u001b[31mE          'surprise': 3,\u001b[0m\n",
      "\u001b[1m\u001b[31mE          'two': 1,\u001b[0m\n",
      "\u001b[1m\u001b[31mE          'weapons': 1,\u001b[0m\n",
      "\u001b[1m\u001b[31mE          'well': 1}\u001b[0m\n",
      "\u001b[1m\u001b[31mE         Full diff:\u001b[0m\n",
      "\u001b[1m\u001b[31mE           {\u001b[0m\n",
      "\u001b[1m\u001b[31mE         -  '\\n': 1,\u001b[0m\n",
      "\u001b[1m\u001b[31mE         -  '\\nWell,': 1,\u001b[0m\n",
      "\u001b[1m\u001b[31mE         -  '...\\nNo': 1,\u001b[0m\n",
      "\u001b[1m\u001b[31mE         -  'I': 1,\u001b[0m\n",
      "\u001b[1m\u001b[31mE         -  'Inquisition': 1,\u001b[0m\n",
      "\u001b[1m\u001b[31mE         -  'Inquisition!\\nOur': 1,\u001b[0m\n",
      "\u001b[1m\u001b[31mE         -  'Spanish': 2,\u001b[0m\n",
      "\u001b[1m\u001b[31mE            'and': 2,\u001b[0m\n",
      "\u001b[1m\u001b[31mE            'chief': 2,\u001b[0m\n",
      "\u001b[1m\u001b[31mE         -  \"didn't\": 1,\u001b[0m\n",
      "\u001b[1m\u001b[31mE         ?  ^     --\u001b[0m\n",
      "\u001b[1m\u001b[31mE         +  'didnt': 1,\u001b[0m\n",
      "\u001b[1m\u001b[31mE         ?  ^    +\u001b[0m\n",
      "\u001b[1m\u001b[31mE         -  'efficiency!': 1,\u001b[0m\n",
      "\u001b[1m\u001b[31mE         ?             -\u001b[0m\n",
      "\u001b[1m\u001b[31mE         +  'efficiency': 1,\u001b[0m\n",
      "\u001b[1m\u001b[31mE            'expected': 1,\u001b[0m\n",
      "\u001b[1m\u001b[31mE            'expects': 1,\u001b[0m\n",
      "\u001b[1m\u001b[31mE         -  'fear': 1,\u001b[0m\n",
      "\u001b[1m\u001b[31mE         ?          ^\u001b[0m\n",
      "\u001b[1m\u001b[31mE         +  'fear': 2,\u001b[0m\n",
      "\u001b[1m\u001b[31mE         ?          ^\u001b[0m\n",
      "\u001b[1m\u001b[31mE         -  'fear,': 1,\u001b[0m\n",
      "\u001b[1m\u001b[31mE         +  'i': 1,\u001b[0m\n",
      "\u001b[1m\u001b[31mE         +  'inquisition': 2,\u001b[0m\n",
      "\u001b[1m\u001b[31mE            'is': 1,\u001b[0m\n",
      "\u001b[1m\u001b[31mE         +  'no': 1,\u001b[0m\n",
      "\u001b[1m\u001b[31mE            'one': 1,\u001b[0m\n",
      "\u001b[1m\u001b[31mE         +  'our': 1,\u001b[0m\n",
      "\u001b[1m\u001b[31mE            'ruthless': 1,\u001b[0m\n",
      "\u001b[1m\u001b[31mE         +  'spanish': 2,\u001b[0m\n",
      "\u001b[1m\u001b[31mE         -  'surprise,': 2,\u001b[0m\n",
      "\u001b[1m\u001b[31mE         ?           -   ^\u001b[0m\n",
      "\u001b[1m\u001b[31mE         +  'surprise': 3,\u001b[0m\n",
      "\u001b[1m\u001b[31mE         ?              ^\u001b[0m\n",
      "\u001b[1m\u001b[31mE         -  'surprise;\\ntwo': 1,\u001b[0m\n",
      "\u001b[1m\u001b[31mE            'the': 2,\u001b[0m\n",
      "\u001b[1m\u001b[31mE         +  'two': 1,\u001b[0m\n",
      "\u001b[1m\u001b[31mE            'weapon': 1,\u001b[0m\n",
      "\u001b[1m\u001b[31mE         -  'weapons,': 1,\u001b[0m\n",
      "\u001b[1m\u001b[31mE         ?          -\u001b[0m\n",
      "\u001b[1m\u001b[31mE         +  'weapons': 1,\u001b[0m\n",
      "\u001b[1m\u001b[31mE         +  'well': 1,\u001b[0m\n",
      "\u001b[1m\u001b[31mE           }\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mtest_spanish_inquisition.py\u001b[0m:29: AssertionError\n",
      "\n",
      " \u001b[36mtest_spanish_inquisition.py\u001b[0m::test_spanish_inquisition\u001b[0m \u001b[31m⨯\u001b[0m         \u001b[31m100% \u001b[0m\u001b[40m\u001b[31m█\u001b[0m\u001b[40m\u001b[31m█████████\u001b[0m\n",
      "\n",
      "Results (0.04s):\n",
      "\u001b[31m       1 failed\u001b[0m\n",
      "         - \u001b[36m\u001b[0mtest_spanish_inquisition.py\u001b[0m:27 \u001b[31mtest_spanish_inquisition\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python -m pytest -vv test_spanish_inquisition.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "jQjm-hif_98X",
    "outputId": "a749dd57-db52-4d42-ff08-8e6b5f8dab41"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting spanish_inquisition.py\n"
     ]
    }
   ],
   "source": [
    "%%file spanish_inquisition.py\n",
    "# version 2: fixed\n",
    "import collections\n",
    "\n",
    "quote = \"\"\"\n",
    "Well, I didn't expected the Spanish Inquisition ...\n",
    "No one expects the Spanish Inquisition!\n",
    "Our chief weapon is surprise, fear and surprise;\n",
    "two chief weapons, fear, surprise, and ruthless efficiency! \n",
    "\"\"\"\n",
    "\n",
    "def remove_punctuation(quote):\n",
    "    # quote.translate(str.maketrans('', '', \"',.!?;\")).lower() # BUG: missing return\n",
    "    return quote.translate(str.maketrans('', '', \"',.!?;\")).lower()\n",
    "\n",
    "def count_words(quote):\n",
    "    quote = remove_punctuation(quote)\n",
    "    # return dict(collections.Counter(quote.split(' '))) # BUG\n",
    "    return dict(collections.Counter(quote.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "colab_type": "code",
    "id": "nI4G32m5AJ_P",
    "outputId": "fab1ee98-a611-443a-da83-2b2898f9442e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mTest session starts (platform: linux, Python 3.6.9, pytest 5.3.5, pytest-sugar 0.9.2)\u001b[0m\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /content\n",
      "plugins: sugar-0.9.2, xdist-1.31.0, forked-1.1.3, timeout-1.3.4\n",
      "\u001b[1mcollecting ... \u001b[0m\n",
      " \u001b[36mtest_spanish_inquisition.py\u001b[0m::test_spanish_inquisition\u001b[0m \u001b[32m✓\u001b[0m         \u001b[32m100% \u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█████████\u001b[0m\n",
      "\n",
      "Results (0.02s):\n",
      "\u001b[32m       1 passed\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python -m pytest -vv test_spanish_inquisition.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v90fn0mbitTu"
   },
   "source": [
    "# Using fixtures to simplify tests\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W1NkKG7PlQyk"
   },
   "source": [
    "## Motivating example\n",
    "\n",
    "Lets look at an example of class `Person`, where each person has a name and remembers their friends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "gNw35je2i9iA",
    "outputId": "9c4d7ae0-236f-4c91-f5da-485cd3c37a9b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting person.py\n"
     ]
    }
   ],
   "source": [
    "%%file person.py\n",
    "\n",
    "#version 1\n",
    "class Person:\n",
    "    def __init__(self, name, favorite_color, year_born):\n",
    "        self.name = name\n",
    "        self.favorite_color = favorite_color\n",
    "        self.year_born = year_born\n",
    "        self.friends = set()\n",
    "\n",
    "    def add_friend(self, other_person):\n",
    "        if not isinstance(other_person, Person): raise TypeError(other_person, 'is not a', Person)\n",
    "        self.friends.add(other_person)\n",
    "        other_person.friends.add(self)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'Person(name={self.name!r}, '  \\\n",
    "               f'favorite_color={self.favorite_color!r}, ' \\\n",
    "               f'year_born={self.year_born!r}, ' \\\n",
    "               f'friends={[f.name for f in self.friends]})'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hqOAgl_olosJ"
   },
   "source": [
    "Lets write a test for `add_friend()` function.\n",
    "\n",
    "notice how the setup for the test is taking so much of the function, while also requiring _inventing_ a lot of repetitious data\n",
    "\n",
    "is there a way to reduce this boiler plate code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "H7kOQpl3j01h",
    "outputId": "07896c96-4249-4a1c-d72c-bfaf3d435cae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting test_person.py\n"
     ]
    }
   ],
   "source": [
    "%%file test_person.py\n",
    "\n",
    "from person import Person\n",
    "\n",
    "def test_name():\n",
    "    # setup\n",
    "    terry = Person(\n",
    "        'Terry Gilliam',\n",
    "        'red',\n",
    "        1940\n",
    "        )\n",
    "    \n",
    "    # test\n",
    "    assert terry.name == 'Terry Gilliam' \n",
    "\n",
    "\n",
    "def test_add_friend():\n",
    "    # setup for the test \n",
    "    terry = Person(\n",
    "        'Terry Gilliam',\n",
    "        'red',\n",
    "        1940\n",
    "        )\n",
    "    eric = Person(\n",
    "        'Eric Idle',\n",
    "        'blue',\n",
    "        1943\n",
    "        )\n",
    "    \n",
    "    # actual test\n",
    "    terry.add_friend(eric)\n",
    "    assert eric in terry.friends\n",
    "    assert terry in eric.friends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "KYMxE55mjt02",
    "outputId": "58448a59-0222-420a-a981-ebc1a81fe952"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..\u001b[36m                                                                       [100%]\u001b[0m\n",
      "\u001b[32m\u001b[1m2 passed in 0.01 seconds\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python -m pytest -q test_person.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pu8SxYwXp0n6"
   },
   "source": [
    "## Fixtures to the rescue\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8G6Yj8hvq4hh"
   },
   "source": [
    "\n",
    "what is we had a magic factory that can conjure up a name, favorite color and birth year?\n",
    "\n",
    "then we could write our `test_name()` more simply like this:\n",
    "\n",
    "```python\n",
    "def test_name(person_name, favorite_color, birth_year):\n",
    "    person = Person(person_name, favorite_color, birth_year)\n",
    "    \n",
    "    # test\n",
    "    assert person.name == person_name \n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AJiA16D9q8Qc"
   },
   "source": [
    "furthermore, if we had a magic factory that can create `terry` and `eric` we could write our `test_add_friend()` function like this:\n",
    "\n",
    "```python\n",
    "def test_add_friend(eric, terry):\n",
    "    eric.add_friend(terry)\n",
    "    assert eric in terry.friends\n",
    "    assert terry in eric.friends\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TmzljhcnrZii"
   },
   "source": [
    "fixtures in `pytest` allow us to create such magic factories using the `@pytest.fixture` notation.\n",
    "\n",
    "here's an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "lfC-AO6PqMIP",
    "outputId": "1d93cd1d-3cef-4e6a-e69f-5ab695205124"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting test_person_fixtures1.py\n"
     ]
    }
   ],
   "source": [
    "%%file test_person_fixtures1.py\n",
    "\n",
    "import pytest\n",
    "from person import Person\n",
    "\n",
    "@pytest.fixture\n",
    "def person_name():\n",
    "    return 'Terry Gilliam'\n",
    "\n",
    "@pytest.fixture\n",
    "def birth_year():\n",
    "    return 1940\n",
    "\n",
    "@pytest.fixture\n",
    "def favorite_color():\n",
    "    return 'red'\n",
    "\n",
    "def test_person_name(person_name, favorite_color, birth_year):\n",
    "    person = Person(person_name, favorite_color, birth_year)\n",
    " \n",
    "    # test\n",
    "    assert person.name == person_name "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 155
    },
    "colab_type": "code",
    "id": "fsg5u51AsS3B",
    "outputId": "d82c7d1d-c076-44cf-db7f-679b714b4532"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.6.9, pytest-3.6.4, py-1.8.1, pluggy-0.7.1\n",
      "rootdir: /content, inifile:\n",
      "collected 1 item                                                               \u001b[0m\u001b[1m\n",
      "\n",
      "test_person_fixtures1.py .\u001b[36m                                               [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m\u001b[1m=========================== 1 passed in 0.02 seconds ===========================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python -m pytest test_person_fixtures1.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q4TIxQSBxYHo"
   },
   "source": [
    "what's happening here?\n",
    "\n",
    "`pytest` sees that the test function `test_person_name(person_name, favorite_color, birth_year)` requires three parameters, and searches for fixtures annotated with `@pytest.fixture` with the same name.\n",
    "\n",
    "when it finds them, it calls these fixtures on our behalf, and passes the return value as the parameter. in effect, it calls\n",
    "\n",
    "```python\n",
    "test_person_name(person_name=person_name(), favorite_color=favorite_color(), birth_year=birth_year()\n",
    "```\n",
    "\n",
    "note how much code this saves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "itQIQgL1p1ub"
   },
   "source": [
    "## your turn\n",
    "1. rewrite the `test_add_friend` function to accept two parameters `def test_add_friend(eric, terry)` \n",
    "2. write fixtures for eric and terry\n",
    "3. run pytest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BwO3ul3PuEI5"
   },
   "source": [
    "## solution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ZgEClKykuFuM",
    "outputId": "069f166a-70a5-461c-d845-59981d540cc6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing test_person_fixtures2.py\n"
     ]
    }
   ],
   "source": [
    "%%file test_person_fixtures2.py\n",
    "\n",
    "import pytest\n",
    "from person import Person\n",
    "\n",
    "@pytest.fixture\n",
    "def eric():\n",
    "    return Person('Eric Idle', 'red', 1943)\n",
    "\n",
    "@pytest.fixture\n",
    "def terry():\n",
    "    return Person('Terry Gilliam', 'blue', 1940)\n",
    "\n",
    "def test_add_friend(eric, terry):\n",
    "    eric.add_friend(terry)\n",
    "    assert eric in terry.friends\n",
    "    assert terry in eric.friends\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "6-OxB-hXu0Gh",
    "outputId": "94f228c4-1566-4ff7-d646-3a90ea5ef4f0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\u001b[36m                                                                        [100%]\u001b[0m\n",
      "\u001b[32m\u001b[1m1 passed in 0.02 seconds\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python -m pytest -q test_person_fixtures2.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JTresMzCxjwc"
   },
   "source": [
    "# parameterizing fixtures\n",
    "\n",
    "Fixture functions can be parametrized in which case they will be called multiple times, each time executing the set of dependent tests, i. e. the tests that depend on this fixture. \n",
    "\n",
    "Test functions usually do not need to be aware of their re-running. Fixture parametrization helps to write exhaustive functional tests for components which themselves can be configured in multiple ways.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Ucbc8ijzxy9q",
    "outputId": "b87d6ff0-bdb5-4f60-bee4-640f6b1c7f7e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting test_primes.py\n"
     ]
    }
   ],
   "source": [
    "%%file test_primes.py\n",
    "\n",
    "import pytest\n",
    "import math\n",
    "\n",
    "def is_prime(x):\n",
    "    return all(x % factor != 0 for factor in range(2, int(x/2)))\n",
    "\n",
    "@pytest.fixture(params=[2,3,5,7,11, 13, 17, 19, 101])\n",
    "def prime_number(request):\n",
    "    return request.param\n",
    "\n",
    "def test_prime(prime_number):\n",
    "    assert is_prime(prime_number) == True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 311
    },
    "colab_type": "code",
    "id": "1OQjbA9T4XIv",
    "outputId": "34626dd4-4b6d-4d19-cd92-9038bb5ccb6f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.6.9, pytest-3.6.4, py-1.8.1, pluggy-0.7.1 -- /usr/bin/python3\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /content, inifile:\n",
      "collected 9 items                                                              \u001b[0m\u001b[1m\n",
      "\n",
      "test_primes.py::test_prime[2] \u001b[32mPASSED\u001b[0m\u001b[36m                                     [ 11%]\u001b[0m\n",
      "test_primes.py::test_prime[3] \u001b[32mPASSED\u001b[0m\u001b[36m                                     [ 22%]\u001b[0m\n",
      "test_primes.py::test_prime[5] \u001b[32mPASSED\u001b[0m\u001b[36m                                     [ 33%]\u001b[0m\n",
      "test_primes.py::test_prime[7] \u001b[32mPASSED\u001b[0m\u001b[36m                                     [ 44%]\u001b[0m\n",
      "test_primes.py::test_prime[11] \u001b[32mPASSED\u001b[0m\u001b[36m                                    [ 55%]\u001b[0m\n",
      "test_primes.py::test_prime[13] \u001b[32mPASSED\u001b[0m\u001b[36m                                    [ 66%]\u001b[0m\n",
      "test_primes.py::test_prime[17] \u001b[32mPASSED\u001b[0m\u001b[36m                                    [ 77%]\u001b[0m\n",
      "test_primes.py::test_prime[19] \u001b[32mPASSED\u001b[0m\u001b[36m                                    [ 88%]\u001b[0m\n",
      "test_primes.py::test_prime[101] \u001b[32mPASSED\u001b[0m\u001b[36m                                   [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m\u001b[1m=========================== 9 passed in 0.03 seconds ===========================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python -m pytest --verbose test_primes.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3hYUoGSR3Ekp"
   },
   "source": [
    "## your turn\n",
    "\n",
    "test `is_prime()` for non prime numbers\n",
    "> bonus: can you find and fix the bug in `is_prime()` using a test?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oSgA7sba3QPp"
   },
   "source": [
    "## solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "gGRatrl_3Uw5",
    "outputId": "128b567b-6e71-4efd-efc4-a1920fe0141d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting test_non_primes.py\n"
     ]
    }
   ],
   "source": [
    "%%file test_non_primes.py\n",
    "\n",
    "import pytest\n",
    "\n",
    "FIX_BUG = True\n",
    "if FIX_BUG:\n",
    "    def is_prime_fixed(x):\n",
    "        # notice the +1 - it is important when x=4\n",
    "        return all(x % factor != 0 for factor in range(2, int(x/2) + 1))\n",
    "    is_prime = is_prime_fixed\n",
    "else:\n",
    "    from test_primes import is_prime\n",
    "\n",
    "@pytest.fixture(params=[4, 6, 8, 9, 10, 12, 14, 15, 16, 28, 60, 100])\n",
    "def non_prime_number(request):\n",
    "    return request.param\n",
    "\n",
    "def test_non_primes(non_prime_number):\n",
    "    assert is_prime(non_prime_number) == False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "colab_type": "code",
    "id": "LbDQVXfC3wyI",
    "outputId": "25375972-0b67-40aa-d957-940d6f1a7078"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.6.9, pytest-3.6.4, py-1.8.1, pluggy-0.7.1 -- /usr/bin/python3\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /content, inifile:\n",
      "collected 12 items                                                             \u001b[0m\u001b[1m\n",
      "\n",
      "test_non_primes.py::test_non_primes[4] \u001b[32mPASSED\u001b[0m\u001b[36m                            [  8%]\u001b[0m\n",
      "test_non_primes.py::test_non_primes[6] \u001b[32mPASSED\u001b[0m\u001b[36m                            [ 16%]\u001b[0m\n",
      "test_non_primes.py::test_non_primes[8] \u001b[32mPASSED\u001b[0m\u001b[36m                            [ 25%]\u001b[0m\n",
      "test_non_primes.py::test_non_primes[9] \u001b[32mPASSED\u001b[0m\u001b[36m                            [ 33%]\u001b[0m\n",
      "test_non_primes.py::test_non_primes[10] \u001b[32mPASSED\u001b[0m\u001b[36m                           [ 41%]\u001b[0m\n",
      "test_non_primes.py::test_non_primes[12] \u001b[32mPASSED\u001b[0m\u001b[36m                           [ 50%]\u001b[0m\n",
      "test_non_primes.py::test_non_primes[14] \u001b[32mPASSED\u001b[0m\u001b[36m                           [ 58%]\u001b[0m\n",
      "test_non_primes.py::test_non_primes[15] \u001b[32mPASSED\u001b[0m\u001b[36m                           [ 66%]\u001b[0m\n",
      "test_non_primes.py::test_non_primes[16] \u001b[32mPASSED\u001b[0m\u001b[36m                           [ 75%]\u001b[0m\n",
      "test_non_primes.py::test_non_primes[28] \u001b[32mPASSED\u001b[0m\u001b[36m                           [ 83%]\u001b[0m\n",
      "test_non_primes.py::test_non_primes[60] \u001b[32mPASSED\u001b[0m\u001b[36m                           [ 91%]\u001b[0m\n",
      "test_non_primes.py::test_non_primes[100] \u001b[32mPASSED\u001b[0m\u001b[36m                          [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m\u001b[1m========================== 12 passed in 0.03 seconds ===========================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python -m pytest --verbose test_non_primes.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ZO-EEPS32u5i",
    "outputId": "e9c8b4c9-eab8-4335-e933-33a940258bf9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 41,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all([factor for factor in range(2, int(4/2))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 708
    },
    "colab_type": "code",
    "id": "ayotSLStzjwx",
    "outputId": "4cabd563-a1d7-471d-a444-27e839060a26"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.6.9, pytest-3.6.4, py-1.8.1, pluggy-0.7.1 -- /usr/bin/python3\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /content, inifile:\n",
      "collected 21 items                                                             \u001b[0m\u001b[1m\n",
      "\n",
      "test_primes.py::test_prime[2] \u001b[32mPASSED\u001b[0m\u001b[36m                                     [  4%]\u001b[0m\n",
      "test_primes.py::test_prime[3] \u001b[32mPASSED\u001b[0m\u001b[36m                                     [  9%]\u001b[0m\n",
      "test_primes.py::test_prime[5] \u001b[32mPASSED\u001b[0m\u001b[36m                                     [ 14%]\u001b[0m\n",
      "test_primes.py::test_prime[7] \u001b[32mPASSED\u001b[0m\u001b[36m                                     [ 19%]\u001b[0m\n",
      "test_primes.py::test_prime[11] \u001b[32mPASSED\u001b[0m\u001b[36m                                    [ 23%]\u001b[0m\n",
      "test_primes.py::test_prime[13] \u001b[32mPASSED\u001b[0m\u001b[36m                                    [ 28%]\u001b[0m\n",
      "test_primes.py::test_prime[17] \u001b[32mPASSED\u001b[0m\u001b[36m                                    [ 33%]\u001b[0m\n",
      "test_primes.py::test_prime[19] \u001b[32mPASSED\u001b[0m\u001b[36m                                    [ 38%]\u001b[0m\n",
      "test_primes.py::test_prime[101] \u001b[32mPASSED\u001b[0m\u001b[36m                                   [ 42%]\u001b[0m\n",
      "test_primes.py::test_non_primes[4] \u001b[31mFAILED\u001b[0m\u001b[36m                                [ 47%]\u001b[0m\n",
      "test_primes.py::test_non_primes[6] \u001b[32mPASSED\u001b[0m\u001b[36m                                [ 52%]\u001b[0m\n",
      "test_primes.py::test_non_primes[8] \u001b[32mPASSED\u001b[0m\u001b[36m                                [ 57%]\u001b[0m\n",
      "test_primes.py::test_non_primes[9] \u001b[32mPASSED\u001b[0m\u001b[36m                                [ 61%]\u001b[0m\n",
      "test_primes.py::test_non_primes[10] \u001b[32mPASSED\u001b[0m\u001b[36m                               [ 66%]\u001b[0m\n",
      "test_primes.py::test_non_primes[12] \u001b[32mPASSED\u001b[0m\u001b[36m                               [ 71%]\u001b[0m\n",
      "test_primes.py::test_non_primes[14] \u001b[32mPASSED\u001b[0m\u001b[36m                               [ 76%]\u001b[0m\n",
      "test_primes.py::test_non_primes[15] \u001b[32mPASSED\u001b[0m\u001b[36m                               [ 80%]\u001b[0m\n",
      "test_primes.py::test_non_primes[16] \u001b[32mPASSED\u001b[0m\u001b[36m                               [ 85%]\u001b[0m\n",
      "test_primes.py::test_non_primes[28] \u001b[32mPASSED\u001b[0m\u001b[36m                               [ 90%]\u001b[0m\n",
      "test_primes.py::test_non_primes[60] \u001b[32mPASSED\u001b[0m\u001b[36m                               [ 95%]\u001b[0m\n",
      "test_primes.py::test_non_primes[100] \u001b[32mPASSED\u001b[0m\u001b[36m                              [100%]\u001b[0m\n",
      "\n",
      "=================================== FAILURES ===================================\n",
      "\u001b[31m\u001b[1m______________________________ test_non_primes[4] ______________________________\u001b[0m\n",
      "\n",
      "non_prime_number = 4\n",
      "\n",
      "\u001b[1m    def test_non_primes(non_prime_number):\u001b[0m\n",
      "\u001b[1m>       assert is_prime(non_prime_number) == False\u001b[0m\n",
      "\u001b[1m\u001b[31mE       assert True == False\u001b[0m\n",
      "\u001b[1m\u001b[31mE        +  where True = is_prime(4)\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mtest_primes.py\u001b[0m:20: AssertionError\n",
      "\u001b[31m\u001b[1m===================== 1 failed, 20 passed in 0.06 seconds ======================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python -m pytest --verbose test_primes.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_HkdBwuXB1QU"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PiaLU-XY5NbQ"
   },
   "source": [
    "# printing and logging within tests\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0GKZD-Ek-_ym"
   },
   "source": [
    "## printing\n",
    "[Reference](https://docs.pytest.org/en/latest/capture.html)\n",
    "\n",
    "You can use prints within tests to provide additional debug info.\n",
    "\n",
    "pytest redirects the output and captured the output of each test. it then:\n",
    "- __suppresses__ the output of all __successful__ tests (for brevity)\n",
    "- __shows__ the output off all __failed__ tests (for debugging)\n",
    "- both `stdout` and `stderr` are captured\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "oXhnkzC-0mM9",
    "outputId": "34812b5c-ff00-4e81-a6a6-26a8ec306c19"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting test_prints.py\n"
     ]
    }
   ],
   "source": [
    "%%file test_prints.py\n",
    "import sys\n",
    "\n",
    "def test_print_success():\n",
    "    print(\n",
    "        \"\"\"\n",
    "        @@@@@@@@@@@@@@@\n",
    "        this statement will NOT be printed\n",
    "        @@@@@@@@@@@@@@@\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "    assert 6*7 == 42\n",
    "\n",
    "def test_print_fail():\n",
    "\n",
    "    print(\n",
    "        \"\"\"\n",
    "        @@@@@@@@@@@@@@@\n",
    "        this statement WILL be printed\n",
    "        @@@@@@@@@@@@@@@\n",
    "        \"\"\"\n",
    "    )\n",
    "    assert True == False\n",
    "\n",
    "\n",
    "def test_stderr_capture_success():\n",
    "    print(\n",
    "        \"\"\"\n",
    "        @@@@@@@@@@@@@@@\n",
    "        this STDERR statement will NOT be printed\n",
    "        @@@@@@@@@@@@@@@\n",
    "        \"\"\", \n",
    "        file=sys.stderr\n",
    "    )\n",
    "     \n",
    "    assert True\n",
    "\n",
    "\n",
    "def test_stderr_capture_fail():\n",
    "    print(\n",
    "        \"\"\"\n",
    "        @@@@@@@@@@@@@@@\n",
    "        this STDERR statement WILL be printed\n",
    "        @@@@@@@@@@@@@@@\n",
    "        \"\"\", \n",
    "        file=sys.stderr\n",
    "    )\n",
    "     \n",
    "    assert False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 812
    },
    "colab_type": "code",
    "id": "bCnf85Ba6lDZ",
    "outputId": "61b69b5e-3911-4726-c356-56e400466e68"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".F.F\u001b[36m                                                                     [100%]\u001b[0m\n",
      "=================================== FAILURES ===================================\n",
      "\u001b[31m\u001b[1m_______________________________ test_print_fail ________________________________\u001b[0m\n",
      "\n",
      "\u001b[1m    def test_print_fail():\u001b[0m\n",
      "\u001b[1m    \u001b[0m\n",
      "\u001b[1m        print(\u001b[0m\n",
      "\u001b[1m            \"\"\"\u001b[0m\n",
      "\u001b[1m            @@@@@@@@@@@@@@@\u001b[0m\n",
      "\u001b[1m            this statement WILL be printed\u001b[0m\n",
      "\u001b[1m            @@@@@@@@@@@@@@@\u001b[0m\n",
      "\u001b[1m            \"\"\"\u001b[0m\n",
      "\u001b[1m        )\u001b[0m\n",
      "\u001b[1m>       assert True == False\u001b[0m\n",
      "\u001b[1m\u001b[31mE       assert True == False\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mtest_prints.py\u001b[0m:23: AssertionError\n",
      "----------------------------- Captured stdout call -----------------------------\n",
      "\n",
      "        @@@@@@@@@@@@@@@\n",
      "        this statement WILL be printed\n",
      "        @@@@@@@@@@@@@@@\n",
      "        \n",
      "\u001b[31m\u001b[1m___________________________ test_stderr_capture_fail ___________________________\u001b[0m\n",
      "\n",
      "\u001b[1m    def test_stderr_capture_fail():\u001b[0m\n",
      "\u001b[1m        print(\u001b[0m\n",
      "\u001b[1m            \"\"\"\u001b[0m\n",
      "\u001b[1m            @@@@@@@@@@@@@@@\u001b[0m\n",
      "\u001b[1m            this STDERR statement WILL be printed\u001b[0m\n",
      "\u001b[1m            @@@@@@@@@@@@@@@\u001b[0m\n",
      "\u001b[1m            \"\"\",\u001b[0m\n",
      "\u001b[1m            file=sys.stderr\u001b[0m\n",
      "\u001b[1m        )\u001b[0m\n",
      "\u001b[1m    \u001b[0m\n",
      "\u001b[1m>       assert False\u001b[0m\n",
      "\u001b[1m\u001b[31mE       assert False\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mtest_prints.py\u001b[0m:49: AssertionError\n",
      "----------------------------- Captured stderr call -----------------------------\n",
      "\n",
      "        @@@@@@@@@@@@@@@\n",
      "        this STDERR statement WILL be printed\n",
      "        @@@@@@@@@@@@@@@\n",
      "        \n",
      "\u001b[31m\u001b[1m2 failed, 2 passed in 0.04 seconds\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python -m pytest -q test_prints.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KVObE-xb_PlT"
   },
   "source": [
    "## logging\n",
    "[Reference](https://docs.pytest.org/en/latest/logging.html)\n",
    "\n",
    "pytest captures log messages of level WARNING or above automatically and displays them in their own section for each failed test in the same manner as captured stdout and stderr.\n",
    "\n",
    "- WARNING and above will displayed for failed tests\n",
    "- INFO and below will not be displayed\n",
    "\n",
    "example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ScR82eTM_iv1",
    "outputId": "56c4efbf-447b-4c88-da71-2503feacdc3d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting test_logging.py\n"
     ]
    }
   ],
   "source": [
    "%%file test_logging.py\n",
    "\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def test_logging_warning_success():\n",
    "    logger.warning('\\n\\n @@@ this will NOT be printed \\n\\n')\n",
    "    assert True\n",
    "\n",
    "def test_logging_warning_fail():\n",
    "    logger.warning('\\n\\n @@@ this WILL be printed @@@ \\n\\n')\n",
    "    assert False\n",
    "\n",
    "def test_logging_info_fail():\n",
    "    logger.info('\\n\\n @@@ this will NOT be printed @@@ \\n\\n')\n",
    "    assert False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 518
    },
    "colab_type": "code",
    "id": "Q0AveLPyAHR3",
    "outputId": "a1b23c00-aaf8-4697-b69e-0eabb0a5cd69"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.6.9, pytest-3.6.4, py-1.8.1, pluggy-0.7.1\n",
      "rootdir: /content, inifile:\n",
      "collected 3 items                                                              \u001b[0m\u001b[1m\n",
      "\n",
      "test_logging.py .FF\u001b[36m                                                      [100%]\u001b[0m\n",
      "\n",
      "=================================== FAILURES ===================================\n",
      "\u001b[31m\u001b[1m__________________________ test_logging_warning_fail ___________________________\u001b[0m\n",
      "\n",
      "\u001b[1m    def test_logging_warning_fail():\u001b[0m\n",
      "\u001b[1m        logger.warning('\\n\\n @@@ this WILL be printed @@@ \\n\\n')\u001b[0m\n",
      "\u001b[1m>       assert False\u001b[0m\n",
      "\u001b[1m\u001b[31mE       assert False\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mtest_logging.py\u001b[0m:12: AssertionError\n",
      "------------------------------ Captured log call -------------------------------\n",
      "test_logging.py             11 WARNING  \n",
      "\n",
      " @@@ this WILL be printed @@@\n",
      "\u001b[31m\u001b[1m____________________________ test_logging_info_fail ____________________________\u001b[0m\n",
      "\n",
      "\u001b[1m    def test_logging_info_fail():\u001b[0m\n",
      "\u001b[1m        logger.info('\\n\\n @@@ this will NOT be printed @@@ \\n\\n')\u001b[0m\n",
      "\u001b[1m>       assert False\u001b[0m\n",
      "\u001b[1m\u001b[31mE       assert False\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mtest_logging.py\u001b[0m:16: AssertionError\n",
      "\u001b[31m\u001b[1m====================== 2 failed, 1 passed in 0.04 seconds ======================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python -m pytest test_logging.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FxVCJnUY5I4N"
   },
   "source": [
    "## your turn\n",
    "\n",
    "We give below an implementation of the _FizzBuzz_ puzzle:\n",
    "> Write a function that returns the numbers from 1 to 100. But for multiples of three returns “Fizz” instead of the number and for the multiples of five returns “Buzz”. For numbers which are multiples of both three and five return “FizzBuzz”.\n",
    "\n",
    "thus this SHOULD be true\n",
    "```python\n",
    ">>> fizzbuzz() # should return the following (abridged) output\n",
    "[1, 2, 'Fizz', 4, 'Buzz', 6, 7, 8, 'Fizz', 'Buzz', 11, 'Fizz', 13, 14, 'FizzBuzz', ... ]\n",
    "```\n",
    "\n",
    "BUT the implementation is buggy. can you write tests for it and fix it?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "colab_type": "code",
    "id": "c9DEbsHGBsUj",
    "outputId": "42cef9b4-a1d4-4d9f-92f5-5c0b10b2c2a3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting fizzbuzz.py\n"
     ]
    }
   ],
   "source": [
    "%%file fizzbuzz.py\n",
    "\n",
    "def is_multiple(n, divisor):\n",
    "    return n % divisor == 0\n",
    "\n",
    "def fizzbuzz():\n",
    "    \"\"\"\n",
    "    expected output: list with elements numbers \n",
    "        [1, 2, 'Fizz', 4, 'Buzz', 6, 7, 8, 'Fizz', 'Buzz', 11, 'Fizz', 13, 14, 'FizzBuzz', ... ]\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    for i in range(100):\n",
    "        if is_multiple(i, 3):\n",
    "            return \"Fizz\"\n",
    "        elif is_multiple(i, 5):\n",
    "            return \"Buzz\"\n",
    "        elif is_multiple(i, 3) and is_multiple(i, 5):\n",
    "            return \"FizzBuzz\"\n",
    "        else:\n",
    "            return i\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NCpyi39tDtOL"
   },
   "source": [
    "## solution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "44bJMzRsD0j2",
    "outputId": "50c454f1-4e11-4564-d12b-3e3878fa757c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting test_fizzbuzz.py\n"
     ]
    }
   ],
   "source": [
    "%%file test_fizzbuzz.py\n",
    "\n",
    "FIX_BUG = 1\n",
    "if not FIX_BUG:\n",
    "    from fizzbuzz import fizzbuzz\n",
    "else:\n",
    "    def fizzbuzz_fixed():\n",
    "        def translate(i):\n",
    "            if i%3 == 0 and i%5 == 0:\n",
    "                return \"FizzBuzz\"\n",
    "            elif i%3 == 0:\n",
    "                return \"Fizz\"\n",
    "            elif i%5 == 0:\n",
    "                return \"Buzz\"\n",
    "            else:\n",
    "                return i\n",
    "\n",
    "        return [translate(i) for i in range(1, 100+1)]\n",
    "\n",
    "    fizzbuzz = fizzbuzz_fixed\n",
    "\n",
    "\n",
    "import pytest\n",
    "@pytest.fixture\n",
    "def fizzbuzz_result():\n",
    "    result = fizzbuzz()\n",
    "    print(result)\n",
    "    return result\n",
    "\n",
    "@pytest.fixture\n",
    "def fizzbuzz_dict(fizzbuzz_result):\n",
    "    return dict(enumerate(fizzbuzz_result, 1))\n",
    "\n",
    "def test_fizzbuzz_len(fizzbuzz_result):\n",
    "    assert len(fizzbuzz_result) == 100\n",
    "\n",
    "def test_fizzbuzz_len(fizzbuzz_result):\n",
    "    assert type(fizzbuzz_result) == list\n",
    "\n",
    "def test_fizzbuzz_first_element(fizzbuzz_dict):\n",
    "    assert fizzbuzz_dict[1] == 1\n",
    "\n",
    "def test_fizzbuzz_3(fizzbuzz_dict):\n",
    "    assert fizzbuzz_dict[3] == 'Fizz'\n",
    "\n",
    "def test_fizzbuzz_5(fizzbuzz_dict):\n",
    "    assert fizzbuzz_dict[5] == 'Buzz'\n",
    "\n",
    "def test_fizzbuzz_15(fizzbuzz_dict):\n",
    "    assert fizzbuzz_dict[15] == 'FizzBuzz'\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 155
    },
    "colab_type": "code",
    "id": "Twogbse2EQvo",
    "outputId": "3fe68876-056f-4ca5-8b1b-5fcbc29b76a8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.6.9, pytest-3.6.4, py-1.8.1, pluggy-0.7.1\n",
      "rootdir: /content, inifile:\n",
      "collected 5 items                                                              \u001b[0m\u001b[1m\n",
      "\n",
      "test_fizzbuzz.py .....\u001b[36m                                                   [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m\u001b[1m=========================== 5 passed in 0.03 seconds ===========================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python -m pytest test_fizzbuzz.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tOyVKYiRLi1Q"
   },
   "source": [
    "# float: when things are (almost) equal\n",
    "[Reference](https://docs.pytest.org/en/latest/reference.html#pytest-approx)\n",
    "\n",
    "consider the following code, what do you expect the result to be?\n",
    "```\n",
    "x = 0.1 + 0.2\n",
    "y = 0.3\n",
    "print('x == y', x ==y) # what will it print?\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "CKLgutHfMH-U",
    "outputId": "cea086a1-339a-43c7-de0a-7f216f94a840"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x == y: False\n"
     ]
    }
   ],
   "source": [
    "x = 0.1 + 0.2\n",
    "y = 0.3\n",
    "print('x == y:', x == y) # what will it print?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EGNsC4VeLlMJ"
   },
   "source": [
    "if you had anticipated `True` it means you haven't tried testing code with `float` data yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "rOj89Un8MZZz",
    "outputId": "089c06a9-8725-46cd-9c32-bafea0b97840"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.30000000000000004 != 0.3\n"
     ]
    }
   ],
   "source": [
    "print(x, '!=', y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UFI1LGmyMpXZ"
   },
   "source": [
    "the issue is that float is _approxiamtely_ accurate (enough for most calculations) but may have small rounding errors.\n",
    "\n",
    "here'e a common but ugly way to test for float equivalence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "6D3uxeokNRxi",
    "outputId": "2dd2ccdf-8c41-40b2-dc45-fdf5c09c1c8a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 136,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abs((0.1 + 0.2) - 0.3) < 1e-6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "S3NPogWSNanZ"
   },
   "source": [
    "here's a more pythonic and pytest-tic way, using `pytest.approx`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "BApEzps0NZ9Y",
    "outputId": "258afdc5-851e-4d2c-a1e7-68fe1cde7dd0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 137,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pytest import approx\n",
    "0.1 + 0.2 == approx(0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Tgiv7X9QNt4G"
   },
   "source": [
    "## your turn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YhtvMq5gPCO7"
   },
   "source": [
    "\n",
    "test that \n",
    "- `math.sin(0) == 0`, \n",
    "- `math.sin(math.pi / 2) == 1`\n",
    "- `math.sin(math.pi) == 0`\n",
    "- `math.sin(math.pi * 3/2) == -1`\n",
    "- `math.sin(math.pi * 2) == 0`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V4vZMr7COM7Z"
   },
   "source": [
    "## solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "JOb7g8BhOO44",
    "outputId": "1bcfabd0-a5ef-4399-f66a-58fbc06cbdbb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting test_sin.py\n"
     ]
    }
   ],
   "source": [
    "%%file test_sin.py\n",
    "\n",
    "from pytest import approx\n",
    "import math\n",
    "def test_sin():\n",
    "    assert math.sin(0) == 0\n",
    "    assert math.sin(math.pi / 2) == 1\n",
    "    assert math.sin(math.pi) == approx(0)\n",
    "    assert math.sin(math.pi * 3/2) == approx(-1)\n",
    "    assert math.sin(math.pi * 2) == approx(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 380
    },
    "colab_type": "code",
    "id": "fLWreR5DOs__",
    "outputId": "b32a0521-fffd-402c-c472-35971f2b35b5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.6.9, pytest-3.6.4, py-1.8.1, pluggy-0.7.1\n",
      "rootdir: /content, inifile:\n",
      "collected 1 item                                                               \u001b[0m\u001b[1m\n",
      "\n",
      "test_sin.py F\u001b[36m                                                            [100%]\u001b[0m\n",
      "\n",
      "=================================== FAILURES ===================================\n",
      "\u001b[31m\u001b[1m___________________________________ test_sin ___________________________________\u001b[0m\n",
      "\n",
      "\u001b[1m    def test_sin():\u001b[0m\n",
      "\u001b[1m        assert math.sin(0) == 0\u001b[0m\n",
      "\u001b[1m        assert math.sin(math.pi / 2) == 1\u001b[0m\n",
      "\u001b[1m>       assert math.sin(math.pi) == 0 #approx(0)\u001b[0m\n",
      "\u001b[1m\u001b[31mE       assert 1.2246467991473532e-16 == 0\u001b[0m\n",
      "\u001b[1m\u001b[31mE        +  where 1.2246467991473532e-16 = <built-in function sin>(3.141592653589793)\u001b[0m\n",
      "\u001b[1m\u001b[31mE        +    where <built-in function sin> = math.sin\u001b[0m\n",
      "\u001b[1m\u001b[31mE        +    and   3.141592653589793 = math.pi\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mtest_sin.py\u001b[0m:7: AssertionError\n",
      "\u001b[31m\u001b[1m=========================== 1 failed in 0.03 seconds ===========================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python -m pytest test_sin.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QC62pY7IlKBx"
   },
   "source": [
    "# adding timeouts to tests\n",
    "[Reference](https://pypi.org/project/pytest-timeout/)\n",
    "\n",
    "Sometimes code gets stuck in an infinite loop, or waiting for a response from a server.\n",
    "Sometimes, tests that run too long is in _itself_ an indication of failure.\n",
    "\n",
    "how can we add timeouts to tests to avoid getting stuck?\n",
    "the package `pytest-timeout` solves for that by providing a plugin to pytest.\n",
    "\n",
    "1. install the package using `pip install pytest-timeout` \n",
    "2. you can set timeouts individually on tests by marking them with the `@pytest.mark.timeout(timeout=60)` decorator\n",
    "3. you can set the timeout for all tests globally by using the timeout commandline parameter for pytest, like so:`pytest --timeout=300`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "loRinVVZlaUe"
   },
   "outputs": [],
   "source": [
    "pip install -q pytest-timeout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Vr_kMCnLlOiT",
    "outputId": "fc82afd9-f14d-4d9f-ea16-cf361e12e5c4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting test_timeouts.py\n"
     ]
    }
   ],
   "source": [
    "%%file test_timeouts.py\n",
    "\n",
    "import pytest\n",
    "\n",
    "@pytest.mark.timeout(5)\n",
    "def test_infinite_sleep():\n",
    "    import time\n",
    "    while True:\n",
    "        time.sleep(1)\n",
    "        print('sleeping ...') \n",
    "\n",
    "def test_empty():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 518
    },
    "colab_type": "code",
    "id": "ikU7sTVUl7dV",
    "outputId": "ae1e4ff9-94b3-4bae-be4c-06e00b32a882"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mTest session starts (platform: linux, Python 3.6.9, pytest 5.3.5, pytest-sugar 0.9.2)\u001b[0m\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /content\n",
      "plugins: sugar-0.9.2, xdist-1.31.0, forked-1.1.3, timeout-1.3.4\n",
      "\u001b[1mcollecting ... \u001b[0m\n",
      "\n",
      "――――――――――――――――――――――――――――― test_infinite_sleep ――――――――――――――――――――――――――――――\n",
      "\n",
      "\u001b[1m    @pytest.mark.timeout(5)\u001b[0m\n",
      "\u001b[1m    def test_infinite_sleep():\u001b[0m\n",
      "\u001b[1m        import time\u001b[0m\n",
      "\u001b[1m        while True:\u001b[0m\n",
      "\u001b[1m>           time.sleep(1)\u001b[0m\n",
      "\u001b[1m\u001b[31mE           Failed: Timeout >5.0s\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mtest_timeouts.py\u001b[0m:8: Failed\n",
      "----------------------------- Captured stdout call -----------------------------\n",
      "sleeping ...\n",
      "sleeping ...\n",
      "sleeping ...\n",
      "sleeping ...\n",
      "\n",
      " \u001b[36mtest_timeouts.py\u001b[0m::test_infinite_sleep\u001b[0m \u001b[31m⨯\u001b[0m                          \u001b[31m50% \u001b[0m\u001b[40m\u001b[31m█\u001b[0m\u001b[40m\u001b[31m████     \u001b[0m\n",
      " \u001b[36mtest_timeouts.py\u001b[0m::test_empty\u001b[0m \u001b[32m✓\u001b[0m                                  \u001b[31m100% \u001b[0m\u001b[40m\u001b[31m█\u001b[0m\u001b[40m\u001b[31m████\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m████\u001b[0m\n",
      "\n",
      "Results (5.03s):\n",
      "\u001b[32m       1 passed\u001b[0m\n",
      "\u001b[31m       1 failed\u001b[0m\n",
      "         - \u001b[36m\u001b[0mtest_timeouts.py\u001b[0m:4 \u001b[31mtest_infinite_sleep\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python -m pytest --verbose test_timeouts.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qHB2vqMln7mJ"
   },
   "source": [
    "notice how the `test_empty` test still runs and passes, even though the previous test was aborted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a8hls8GrqTwp"
   },
   "source": [
    "## your turn\n",
    "\n",
    "1. use the `requests` module to `.get()` the url http://httpstat.us/101 and call `.raise_for_status()`\n",
    "2. since this will hang forever, use a timeout on the test so that it fails after 5 seconds\n",
    "3. since the test is guranteed to fail, mark it with the `xfail` (_expected fail_) annotation `@pytest.mark.xfail(reason='timeout')`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "aPBfhTmnqoay",
    "outputId": "c4e661bf-3f9e-4137-f780-6affcac49b54"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting test_http101_timeout.py\n"
     ]
    }
   ],
   "source": [
    "%%file test_http101_timeout.py\n",
    "\n",
    "import pytest\n",
    "import requests\n",
    "\n",
    "@pytest.mark.xfail(reason='timeout')\n",
    "@pytest.mark.timeout(2)\n",
    "def test_http101_timeout():\n",
    "    response = requests.get('http://httpstat.us/101')\n",
    "    response.raise_for_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 155
    },
    "colab_type": "code",
    "id": "Muj2guSLrDDf",
    "outputId": "11f7721b-b174-41ad-82d0-4954e9e22f80"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mTest session starts (platform: linux, Python 3.6.9, pytest 5.3.5, pytest-sugar 0.9.2)\u001b[0m\n",
      "rootdir: /content\n",
      "plugins: sugar-0.9.2, xdist-1.31.0, forked-1.1.3, timeout-1.3.4\n",
      "\u001b[1mcollecting ... \u001b[0m\n",
      " \u001b[36m\u001b[0mtest_http101_timeout.py\u001b[0m \u001b[32mx\u001b[0m                                       \u001b[32m100% \u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█████████\u001b[0m\n",
      "\n",
      "Results (5.22s):\n",
      "\u001b[32m       1 xfailed\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python -m pytest test_http101_timeout.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ld15jWJZcZ1A"
   },
   "source": [
    "# testing for exceptions\n",
    "[Reference](https://docs.pytest.org/en/3.0.1/assert.html#assertions-about-expected-exceptions)\n",
    "\n",
    "consider the following code fragment from `person.py`:\n",
    "\n",
    "```python\n",
    "class Person:\n",
    "    def add_friend(self, other_person):\n",
    "        if not isinstance(other_person, Person) raise TypeError(other_person, 'is not a', Person)\n",
    "        self.friends.add(other_person)\n",
    "        other_person.friends.add(self)\n",
    "```\n",
    "\n",
    "the `add_friend()` method will raise an exception if it is used with a parameter which is not a `Person`\n",
    "\n",
    "how can we test this?\n",
    "\n",
    "if we wrap the code that is supposed to throw the exc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "a_JMpDtFfOlO",
    "outputId": "3d0e65c5-973a-47e5-96f8-a7ce0705b317"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting test_add_person_exception.py\n"
     ]
    }
   ],
   "source": [
    "%%file test_add_person_exception.py\n",
    "\n",
    "from person import Person\n",
    "from test_person_fixtures2 import *\n",
    "\n",
    "def test_add_person_exception(terry):\n",
    "    with pytest.raises(TypeError):\n",
    "        terry.add_friend(\"a shrubbey!\")\n",
    "\n",
    "def test_add_person_exception_detailed(terry):\n",
    "    with pytest.raises(TypeError) as excinfo:\n",
    "        terry.add_friend(\"a shrubbey!\")\n",
    "    \n",
    "    assert 'Person' in str(excinfo.value)\n",
    "\n",
    "@pytest.mark.xfail(reason='expected to fail')\n",
    "def test_add_person_no_exception(terry, eric):\n",
    "    with pytest.raises(TypeError): # is expecting an exception that won't happen\n",
    "        terry.add_friend(eric) # this does not throw an exception\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "colab_type": "code",
    "id": "7V9YlP82f33w",
    "outputId": "6a47fedb-ed06-4f90-853e-b590c4faea79"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mTest session starts (platform: linux, Python 3.6.9, pytest 5.3.5, pytest-sugar 0.9.2)\u001b[0m\n",
      "rootdir: /content\n",
      "plugins: sugar-0.9.2, xdist-1.31.0, forked-1.1.3, timeout-1.3.4\n",
      "\u001b[1mcollecting ... \u001b[0m\n",
      " \u001b[36m\u001b[0mtest_add_person_exception.py\u001b[0m \u001b[32m✓\u001b[0m\u001b[32m✓\u001b[0m\u001b[32mx\u001b[0m\u001b[32m✓\u001b[0m                               \u001b[32m100% \u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m██\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m██\u001b[0m\n",
      "\n",
      "Results (0.04s):\n",
      "\u001b[32m       3 passed\u001b[0m\n",
      "\u001b[32m       1 xfailed\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python -m pytest test_add_person_exception.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aPuuwkvRiZX-"
   },
   "source": [
    "## your turn\n",
    "use the `requests` module and the `.raise_for_status()` method\n",
    "\n",
    "1. test that `.raise_for_status` will raise an exception when accessing the following URLs:\n",
    "   - http://httpstat.us/401\n",
    "   - http://httpstat.us/404\n",
    "   - http://httpstat.us/500\n",
    "   - http://httpstat.us/501\n",
    "2. test that `.raise_for_status` will NOT raise an exception when accessing the following URLs:\n",
    "   - http://httpstat.us/200\n",
    "   - http://httpstat.us/201\n",
    "   - http://httpstat.us/202\n",
    "   - http://httpstat.us/203\n",
    "   - http://httpstat.us/204\n",
    "   - http://httpstat.us/303\n",
    "   - http://httpstat.us/304  \n",
    "\n",
    "### hints:\n",
    "1. the `requests` module raises exceptions of type `requests.HTTPError`\n",
    "1. use parameterized fixtures to avoid writing a lot of tests or boilerplate code\n",
    "2. use timeouts to avoid tests that wait forever\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A692WcjLuLWe"
   },
   "source": [
    "## solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "8wX1Rfn7jw5u",
    "outputId": "91d8ec3f-ff8d-48be-b0b3-895424c8497c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting test_requests.py\n"
     ]
    }
   ],
   "source": [
    "%%file test_requests.py\n",
    "\n",
    "import pytest\n",
    "import requests\n",
    "\n",
    "@pytest.fixture(params=[200, 201, 202, 203, 204, 303, 304])\n",
    "def good_url(request):\n",
    "    return f'http://httpstat.us/{request.param}'\n",
    "\n",
    "@pytest.fixture(params=[401, 404, 500, 501])\n",
    "def bad_url(request):\n",
    "    return f'http://httpstat.us/{request.param}'\n",
    "\n",
    "@pytest.mark.timeout(2)\n",
    "def test_good_urls(good_url):\n",
    "    response = requests.get(good_url)\n",
    "    response.raise_for_status()\n",
    "\n",
    "@pytest.mark.timeout(2)\n",
    "def test_bad_urls(bad_url):\n",
    "    response = requests.get(bad_url)\n",
    "    with pytest.raises(requests.HTTPError):\n",
    "        response.raise_for_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 314
    },
    "colab_type": "code",
    "id": "O586GEYpzR98",
    "outputId": "f18df864-7d47-4473-ebe8-490a6614961e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pytest-sugar\n",
      "  Downloading https://files.pythonhosted.org/packages/da/3b/f1e3c8830860c1df8f0e0f6713932475141210cfa021e362ca2774d2bf02/pytest_sugar-0.9.2-py2.py3-none-any.whl\n",
      "Requirement already satisfied: packaging>=14.1 in /usr/local/lib/python3.6/dist-packages (from pytest-sugar) (20.1)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from pytest-sugar) (1.1.0)\n",
      "Requirement already satisfied: pytest>=2.9 in /usr/local/lib/python3.6/dist-packages (from pytest-sugar) (5.3.5)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging>=14.1->pytest-sugar) (2.4.6)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging>=14.1->pytest-sugar) (1.12.0)\n",
      "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from pytest>=2.9->pytest-sugar) (8.2.0)\n",
      "Requirement already satisfied: pluggy<1.0,>=0.12 in /usr/local/lib/python3.6/dist-packages (from pytest>=2.9->pytest-sugar) (0.13.1)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from pytest>=2.9->pytest-sugar) (0.1.8)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from pytest>=2.9->pytest-sugar) (19.3.0)\n",
      "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from pytest>=2.9->pytest-sugar) (1.8.1)\n",
      "Requirement already satisfied: importlib-metadata>=0.12; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from pytest>=2.9->pytest-sugar) (1.5.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.12; python_version < \"3.8\"->pytest>=2.9->pytest-sugar) (2.2.0)\n",
      "Installing collected packages: pytest-sugar\n",
      "Successfully installed pytest-sugar-0.9.2\n"
     ]
    }
   ],
   "source": [
    "pip install pytest-sugar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 345
    },
    "colab_type": "code",
    "id": "4KdmZUkWktL0",
    "outputId": "ccaed18d-88b9-40c4-deb8-8e323cbf7064"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mTest session starts (platform: linux, Python 3.6.9, pytest 5.3.5, pytest-sugar 0.9.2)\u001b[0m\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /content\n",
      "plugins: sugar-0.9.2, xdist-1.31.0, forked-1.1.3, timeout-1.3.4\n",
      "\u001b[1mcollecting ... \u001b[0m\n",
      " \u001b[36mtest_requests.py\u001b[0m::test_good_urls[200]\u001b[0m \u001b[32m✓\u001b[0m                           \u001b[32m9% \u001b[0m\u001b[40m\u001b[32m▉\u001b[0m\u001b[40m\u001b[32m         \u001b[0m\n",
      " \u001b[36mtest_requests.py\u001b[0m::test_good_urls[201]\u001b[0m \u001b[32m✓\u001b[0m                          \u001b[32m18% \u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m▊        \u001b[0m\n",
      " \u001b[36mtest_requests.py\u001b[0m::test_good_urls[202]\u001b[0m \u001b[32m✓\u001b[0m                          \u001b[32m27% \u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m▊       \u001b[0m\n",
      " \u001b[36mtest_requests.py\u001b[0m::test_good_urls[203]\u001b[0m \u001b[32m✓\u001b[0m                          \u001b[32m36% \u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m▋      \u001b[0m\n",
      " \u001b[36mtest_requests.py\u001b[0m::test_good_urls[204]\u001b[0m \u001b[32m✓\u001b[0m                          \u001b[32m45% \u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m▋     \u001b[0m\n",
      " \u001b[36mtest_requests.py\u001b[0m::test_good_urls[303]\u001b[0m \u001b[32m✓\u001b[0m                          \u001b[32m55% \u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m▌    \u001b[0m\n",
      " \u001b[36mtest_requests.py\u001b[0m::test_good_urls[304]\u001b[0m \u001b[32m✓\u001b[0m                          \u001b[32m64% \u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m▍   \u001b[0m\n",
      " \u001b[36mtest_requests.py\u001b[0m::test_bad_urls[401]\u001b[0m \u001b[32m✓\u001b[0m                           \u001b[32m73% \u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m▍  \u001b[0m\n",
      " \u001b[36mtest_requests.py\u001b[0m::test_bad_urls[404]\u001b[0m \u001b[32m✓\u001b[0m                           \u001b[32m82% \u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m▎ \u001b[0m\n",
      " \u001b[36mtest_requests.py\u001b[0m::test_bad_urls[500]\u001b[0m \u001b[32m✓\u001b[0m                           \u001b[32m91% \u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m▏\u001b[0m\n",
      " \u001b[36mtest_requests.py\u001b[0m::test_bad_urls[501]\u001b[0m \u001b[32m✓\u001b[0m                          \u001b[32m100% \u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\n",
      "\n",
      "Results (2.12s):\n",
      "\u001b[32m      11 passed\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python -m pytest --verbose test_requests.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RW4Vq1ONjktF"
   },
   "source": [
    "# running tests in parallel\n",
    "\n",
    "[Reference](https://pypi.org/project/pytest-xdist/)\n",
    "\n",
    "The `pytest-xdist` plugin extends pytest with some unique test execution modes:\n",
    "\n",
    "- **test run parallelization**: if you have multiple CPUs or hosts you can use those for a combined test run. This allows to speed up development or to use special resources of remote machines.\n",
    "- **--looponfail**: run your tests repeatedly in a subprocess. After each run pytest waits until a file in your project changes and then re-runs the previously failing tests. This is repeated until all tests pass after which again a full run is performed.\n",
    "- **Multi-Platform coverage**: you can specify different Python interpreters or different platforms and run tests in parallel on all of them.\n",
    "- **--boxed** and **pytest-forked**: running each test in its own process, so that if a test catastrophically crashes, it doesn't interfere with other tests\n",
    "\n",
    "We're going to cover only test run parallelization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A1ur1uHMv29W"
   },
   "source": [
    "first, lets install `pytest-xdist`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XwNCMTHhvpDw"
   },
   "outputs": [],
   "source": [
    "pip install -qq pytest-xdist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v6sd_3BXv63z"
   },
   "source": [
    "now, lets write a few long running tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "UygEsQ23vy0w",
    "outputId": "fd7ee9b2-c70e-4398-d099-e4c770bbb48b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing test_parallel.py\n"
     ]
    }
   ],
   "source": [
    "%%file test_parallel.py\n",
    "\n",
    "import time\n",
    "def test_t1():\n",
    "    time.sleep(2)\n",
    "\n",
    "def test_t2():\n",
    "    time.sleep(2)\n",
    "\n",
    "def test_t3():\n",
    "    time.sleep(2)\n",
    "\n",
    "def test_t4():\n",
    "    time.sleep(2)\n",
    "\n",
    "def test_t5():\n",
    "    time.sleep(2)\n",
    "\n",
    "def test_t6():\n",
    "    time.sleep(2)\n",
    "\n",
    "def test_t7():\n",
    "    time.sleep(2)\n",
    "\n",
    "def test_t8():\n",
    "    time.sleep(2)\n",
    "\n",
    "def test_t9():\n",
    "    time.sleep(2)\n",
    "\n",
    "def test_t10():\n",
    "    time.sleep(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uI-BoKN-xNhl"
   },
   "source": [
    "now, we can run these tests in parallel using the `pytest -n NUM` commandline parameter.\n",
    "\n",
    "Lets use 10 threads, this will allow us to finish in 2 seconds rather than 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 138
    },
    "colab_type": "code",
    "id": "9GfcAn2twcrl",
    "outputId": "bad9a76a-74c4-4ccf-967e-07cae9ba4866"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.6.9, pytest-5.3.5, py-1.8.1, pluggy-0.13.1\n",
      "rootdir: /content\n",
      "plugins: xdist-1.31.0, forked-1.1.3, timeout-1.3.4\n",
      "gw0 [10] / gw1 [10] / gw2 [10] / gw3 [10] / gw4 [10] / gw5 [10] / gw6 [10] / gw7 [10] / gw8 [10] / gw9 [10]\u001b[0m\n",
      "\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m                                                               [100%]\u001b[0m\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m10 passed\u001b[0m\u001b[32m in 5.94s\u001b[0m\u001b[32m ==============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python -m pytest -n 10 test_parallel.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A0AYxisCVDHa"
   },
   "source": [
    "# Codebase to test: class Person\n",
    "\n",
    "Lets reuse the `Person` and `OlympicRunner` classes we've defined in earlier chapters in order to see how to write tests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "xoLNB6529y7l",
    "outputId": "5966f5b4-5090-4d43-ea08-6744455b48d3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting person.py\n"
     ]
    }
   ],
   "source": [
    "%%file person.py\n",
    "\n",
    "# Person v1\n",
    "class Person:\n",
    "    def __init__(self, name):\n",
    "        name = name\n",
    "    def __repr__(self):\n",
    "        return f\"{type(self).__name__}({self.name!r})\"\n",
    "    def walk(self):\n",
    "        print(self.name, 'walking')\n",
    "    def run(self):\n",
    "        print(self.name,'running')\n",
    "    def swim(self):\n",
    "        print(self.name,'swimming')\n",
    "        \n",
    "class OlympicRunner(Person):\n",
    "    def run(self):\n",
    "        print(self.name,self.name,\"running incredibly fast!\")\n",
    "        \n",
    "    def show_medals(self):\n",
    "        print(self.name, 'showing my olympic medals')\n",
    "    \n",
    "def train(person):\n",
    "    person.walk()\n",
    "    person.swim()\n",
    "    person.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TbPMcdlA-spP"
   },
   "source": [
    "# our first test\n",
    "\n",
    "- [conventions](https://docs.pytest.org/en/latest/goodpractices.html#conventions-for-python-test-discovery) \n",
    "  1. files with tests should be called `test_*.py` or `*_test.py `\n",
    "  2. test function name should start with `test_`\n",
    "\n",
    "- to see if our code works, we can use the `assert` python keyword. pytest adds hooks to assertions to make them more useful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "XeURkKHH-qIk",
    "outputId": "5614f16a-dc9a-42a3-d486-4e7a57af3cd5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting test_person1.py\n"
     ]
    }
   ],
   "source": [
    "%%file test_person1.py\n",
    "from person import Person\n",
    "\n",
    "# our first test\n",
    "def test_preson_name():\n",
    "    terry = Person('Terry Gilliam')\n",
    "    assert terry.name == 'Terry Gilliam'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 311
    },
    "colab_type": "code",
    "id": "K7vECkgGL5Y9",
    "outputId": "840e30da-f11b-4ff4-cd64-064141fe9099"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.6.9, pytest-3.6.4, py-1.8.1, pluggy-0.7.1\n",
      "rootdir: /content, inifile:\n",
      "collected 1 item                                                               \u001b[0m\u001b[1m\n",
      "\n",
      "test_person1.py F\u001b[36m                                                        [100%]\u001b[0m\n",
      "\n",
      "=================================== FAILURES ===================================\n",
      "\u001b[31m\u001b[1m_______________________________ test_preson_name _______________________________\u001b[0m\n",
      "\n",
      "\u001b[1m    def test_preson_name():\u001b[0m\n",
      "\u001b[1m        terry = Person('Terry Gilliam')\u001b[0m\n",
      "\u001b[1m>       assert terry.name == 'Terry Gilliam'\u001b[0m\n",
      "\u001b[1m\u001b[31mE       AttributeError: 'Person' object has no attribute 'name'\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mtest_person1.py\u001b[0m:6: AttributeError\n",
      "\u001b[31m\u001b[1m=========================== 1 failed in 0.03 seconds ===========================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python -m pytest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RiicargLAClz"
   },
   "source": [
    "## lets run our tests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "id": "c8F4LBwwAFLf",
    "outputId": "8dbd169b-6f3a-4c66-bee5-95a6174237e8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: file not found: adv python 08 - test driven development.ipynb\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# execute the tests via pytest, arguments are passed to pytest\n",
    "ipytest.run('-qq')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QAhNkTlV_yXT"
   },
   "source": [
    "## running our first test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "661ExV-V9y7p"
   },
   "outputs": [],
   "source": [
    "# very simple test\n",
    "def test_person_repr1():\n",
    "    assert str(Person('terry gilliam')) == f\"Person('terry gilliam')\"\n",
    "\n",
    "# test using mock object\n",
    "def test_train1():\n",
    "    person = mocking.Mock()\n",
    "    \n",
    "    train(person)\n",
    "    person.walk.assert_called_once()\n",
    "    person.run.assert_called_once()\n",
    "    person.swim.assert_called_once()\n",
    "\n",
    "# create factory for person's name\n",
    "@pytest.fixture\n",
    "def person_name():\n",
    "    return 'terry gilliam'\n",
    "    \n",
    "# create factory for Person, that requires a person_name \n",
    "@pytest.fixture\n",
    "def person(person_name):\n",
    "    return Person(person_name)\n",
    "\n",
    "# test using mock object\n",
    "def test_train2(person):\n",
    "    # this makes sure no other method is called\n",
    "    person = mocking.create_autospec(person)\n",
    "    \n",
    "    train(person)\n",
    "    person.walk.assert_called_once()\n",
    "    person.run.assert_called_once()\n",
    "    person.swim.assert_called_once()\n",
    "\n",
    "\n",
    "# test Person using and request a person, person_name from the fixtures\n",
    "def test_person_repr2(person, person_name):\n",
    "    assert str(person) == f\"Person('{person_name}')\"\n",
    "    \n",
    "# fixture with multiple values\n",
    "@pytest.fixture(params=['usain bolt', 'Matthew Wells'])\n",
    "def olympic_runner_name(request):\n",
    "    return request.param\n",
    "\n",
    "@pytest.fixture\n",
    "def olympic_runner(olympic_runner_name):\n",
    "    return OlympicRunner(olympic_runner_name)\n",
    "\n",
    "# test train() using mock object for print\n",
    "@mocking.patch('builtins.print')\n",
    "def test_train3(mocked_print, olympic_runner):\n",
    "    train(olympic_runner)\n",
    "    mocked_print.assert_called()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PV8jU6b29y7t",
    "outputId": "0d737fdc-a703-4003-8cde-2a9be68d7dc9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "......                                                                                                           [100%]\n"
     ]
    }
   ],
   "source": [
    "# execute the tests via pytest, arguments are passed to pytest\n",
    "ipytest.run('-qq')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "08-test_driven_development.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
